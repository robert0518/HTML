from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.impute import KNNImputer
from sklearn.metrics import classification_report, accuracy_score
from sklearn.model_selection import GridSearchCV
import pandas as pd

# Load training, validation, and test data
train_file_path = 'stage2_train_split.csv'
valid_file_path = 'stage2_valid_split.csv'
test_file_path = 'stage2_data/2024_test_data.csv'

train_data = pd.read_csv(train_file_path)
valid_data = pd.read_csv(valid_file_path)
test_data = pd.read_csv(test_file_path)

# Target column for training and validation
train_data['home_team_win'] = train_data['home_team_win'].astype(int)
valid_data['home_team_win'] = valid_data['home_team_win'].astype(int)
y_train = train_data['home_team_win']
y_valid = valid_data['home_team_win']

# Convert date to datetime for training and validation
train_data['date'] = pd.to_datetime(train_data['date'])
valid_data['date'] = pd.to_datetime(valid_data['date'])

# Define linear weighting scheme
def linear_weighting(data):
    max_days = (data['date'] - data['date'].min()).dt.days.max()
    print(f"Max days: {max_days}")
    return (1+(data['date'] - data['date'].min()).dt.days / max_days)


sample_weights = linear_weighting(train_data)
print(f"Sample weights: {sample_weights}")

# Prepare features (one-hot encoding)
categorical_columns = ['is_night_game', 'home_team_abbr', 'away_team_abbr', 'home_team_season', 'away_team_season']
combined_data = pd.concat([
    train_data.drop(columns=['Unnamed: 0', 'id', 'home_pitcher', 'away_pitcher', 'home_team_win', 'date']),
    valid_data.drop(columns=['Unnamed: 0', 'id', 'home_pitcher', 'away_pitcher', 'home_team_win', 'date']),
    test_data.drop(columns=['id', 'home_pitcher', 'away_pitcher'])
], ignore_index=True)
combined_data = pd.get_dummies(combined_data, columns=categorical_columns)

# Split into training, validation, and test sets
X_train = combined_data.iloc[:len(train_data), :].copy()
X_valid = combined_data.iloc[len(train_data):len(train_data) + len(valid_data), :].copy()
X_test = combined_data.iloc[len(train_data) + len(valid_data):, :].copy()

# Apply KNN imputation
knn_imputer = KNNImputer(n_neighbors=5, weights='uniform')
X_train_imputed = knn_imputer.fit_transform(X_train)
X_valid_imputed = knn_imputer.fit_transform(X_valid)
X_test_imputed = knn_imputer.fit_transform(X_test)

# Convert back to DataFrame
X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)
X_valid = pd.DataFrame(X_valid_imputed, columns=X_valid.columns)
X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)

# Define the base estimator as a decision stump
base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)

# Train a baseline AdaBoost model to determine feature importances
baseline_model = AdaBoostClassifier(estimator=base_estimator, random_state=42, n_estimators=1000, algorithm='SAMME')
baseline_model.fit(X_train_imputed, y_train, sample_weight=sample_weights)

# Extract feature importances
feature_importances = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': baseline_model.feature_importances_
}).sort_values(by='Importance', ascending=False).reset_index(drop=True)

print("Top 10 Features by Importance:")
print(feature_importances.head(10))

# Select top features (e.g., top 200)
selected_features = feature_importances.head(200)['Feature']
X_train_selected = X_train[selected_features]
X_valid_selected = X_valid[selected_features]
X_test_selected = X_test[selected_features]

# Hyperparameter tuning for AdaBoost
param_grid = {
    'n_estimators': [1000, 3000, 5000, 7000],
    'learning_rate': [0.01, 0.1, 1.0, 10.0]
}

grid_search = GridSearchCV(
    AdaBoostClassifier(estimator=base_estimator, random_state=42, algorithm='SAMME'),
    param_grid,
    cv=3,
    scoring='accuracy',
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train_selected, y_train, sample_weight=sample_weights)

# Best model
best_adaboost_model = grid_search.best_estimator_
print("Best Parameters:", grid_search.best_params_)

# Validate the model
y_valid_pred = best_adaboost_model.predict(X_valid_selected)
valid_accuracy = accuracy_score(y_valid, y_valid_pred)
valid_classification_metrics = classification_report(y_valid, y_valid_pred)

print(f"Validation Accuracy with AdaBoost: {valid_accuracy}")
print("Classification Report:")
print(valid_classification_metrics)

# Predict on the test set
test_predictions = best_adaboost_model.predict(X_test_selected)
output = pd.DataFrame({
    'id': test_data['id'],
    'home_team_win': test_predictions.astype(bool)
})

best_learning_rate = grid_search.best_params_['learning_rate']
best_n_estimators = grid_search.best_params_['n_estimators']
# Save predictions
output.to_csv(f'stage2_Answer{best_learning_rate}_{best_n_estimators}.csv', index=False)
print(f"\nTest predictions saved to 'Answer{best_learning_rate}_{best_n_estimators}.csv'.")
